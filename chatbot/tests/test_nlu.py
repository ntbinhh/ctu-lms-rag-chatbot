"""
Test NLU Performance for Vietnamese Chatbot
Ki·ªÉm tra hi·ªáu su·∫•t NLU cho chatbot ti·∫øng Vi·ªát
"""

import asyncio
import json
import pytest
from typing import Dict, List, Tuple
from rasa.core.agent import Agent
from rasa.shared.nlu.training_data.training_data import TrainingData
from rasa.shared.nlu.training_data.message import Message
from pathlib import Path
import os

class NLUTester:
    """L·ªõp ki·ªÉm th·ª≠ NLU cho chatbot"""
    
    def __init__(self, model_path: str = "models/"):
        self.model_path = model_path
        self.agent = None
        self.test_cases = self._create_test_cases()
        
    async def load_agent(self):
        """T·∫£i RASA agent"""
        try:
            self.agent = Agent.load(self.model_path)
            print("‚úÖ Agent loaded successfully")
        except Exception as e:
            print(f"‚ùå Failed to load agent: {e}")
            raise
            
    def _create_test_cases(self) -> List[Dict]:
        """T·∫°o test cases d·ª±a tr√™n c√°c intent trong nlu.yml"""
        return [
            # Greet intent tests
            {"text": "xin ch√†o", "expected_intent": "greet"},
            {"text": "ch√†o b·∫°n", "expected_intent": "greet"},
            {"text": "hello", "expected_intent": "greet"},
            {"text": "hi", "expected_intent": "greet"},
            {"text": "ch√†o bu·ªïi s√°ng", "expected_intent": "greet"},
            {"text": "good morning", "expected_intent": "greet"},
            {"text": "h·∫ø l√¥", "expected_intent": "greet"},
            
            # Goodbye intent tests
            {"text": "t·∫°m bi·ªát", "expected_intent": "goodbye"},
            {"text": "bye", "expected_intent": "goodbye"},
            {"text": "goodbye", "expected_intent": "goodbye"},
            {"text": "ch√†o t·∫°m bi·ªát", "expected_intent": "goodbye"},
            {"text": "see you later", "expected_intent": "goodbye"},
            {"text": "ch√∫c ng·ªß ngon", "expected_intent": "goodbye"},
            {"text": "h·∫πn g·∫∑p l·∫°i", "expected_intent": "goodbye"},
            
            # General question intent tests
            {"text": "th√¥ng tin h·ªçc b·ªïng", "expected_intent": "general_question"},
            {"text": "h·ªçc ph√≠ m·ªôt h·ªçc k·ª≥ l√† bao nhi√™u?", "expected_intent": "general_question"},
            {"text": "quy ƒë·ªãnh v·ªÅ ƒëi·ªÉm s·ªë nh∆∞ th·∫ø n√†o?", "expected_intent": "general_question"},
            {"text": "th·ªß t·ª•c xin h·ªçc b·ªïng", "expected_intent": "general_question"},
            {"text": "ƒë·ªãa ch·ªâ tr∆∞·ªùng", "expected_intent": "general_question"},
            {"text": "ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o", "expected_intent": "general_question"},
            {"text": "ƒëi·ªÅu ki·ªán t·ªët nghi·ªáp", "expected_intent": "general_question"},
            {"text": "quy ch·∫ø ƒë√†o t·∫°o", "expected_intent": "general_question"},
            
            # Affirm intent tests
            {"text": "yes", "expected_intent": "affirm"},
            {"text": "ƒë√∫ng", "expected_intent": "affirm"},
            {"text": "ok", "expected_intent": "affirm"},
            {"text": "ƒë∆∞·ª£c", "expected_intent": "affirm"},
            {"text": "ch·∫Øc ch·∫Øn", "expected_intent": "affirm"},
            
            # Deny intent tests
            {"text": "no", "expected_intent": "deny"},
            {"text": "kh√¥ng", "expected_intent": "deny"},
            {"text": "kh√¥ng mu·ªën", "expected_intent": "deny"},
            {"text": "th√¥i", "expected_intent": "deny"},
            
            # Mood great intent tests
            {"text": "tuy·ªát v·ªùi", "expected_intent": "mood_great"},
            {"text": "perfect", "expected_intent": "mood_great"},
            {"text": "qu√° tuy·ªát", "expected_intent": "mood_great"},
            {"text": "t√¥i r·∫•t vui", "expected_intent": "mood_great"},
            
            # Mood unhappy intent tests
            {"text": "t√¥i bu·ªìn", "expected_intent": "mood_unhappy"},
            {"text": "kh√¥ng vui", "expected_intent": "mood_unhappy"},
            {"text": "my day was horrible", "expected_intent": "mood_unhappy"},
            {"text": "stress", "expected_intent": "mood_unhappy"},
            
            # Bot challenge intent tests
            {"text": "b·∫°n l√† ai?", "expected_intent": "bot_challenge"},
            {"text": "are you a bot?", "expected_intent": "bot_challenge"},
            {"text": "b·∫°n c√≥ ph·∫£i l√† bot kh√¥ng?", "expected_intent": "bot_challenge"},
            
            # Schedule related intents
            {"text": "l·ªãch h·ªçc h√¥m nay", "expected_intent": "ask_today_schedule"},
            {"text": "h√¥m nay t√¥i c√≥ m√¥n g√¨", "expected_intent": "ask_today_schedule"},
            
            {"text": "l·ªãch h·ªçc tu·∫ßn n√†y", "expected_intent": "ask_schedule_week"},
            {"text": "th·ªùi kh√≥a bi·ªÉu tu·∫ßn n√†y", "expected_intent": "ask_schedule_week"},
            
            {"text": "l·ªãch h·ªçc ng√†y mai", "expected_intent": "ask_schedule_tomorrow"},
            {"text": "mai c√≥ l·ªõp kh√¥ng", "expected_intent": "ask_schedule_tomorrow"},
            
            {"text": "ti·∫øt h·ªçc ti·∫øp theo", "expected_intent": "ask_next_class"},
            {"text": "m√¥n ti·∫øp theo l√† g√¨", "expected_intent": "ask_next_class"},
            
            {"text": "ctdt", "expected_intent": "ask_student_program"},
            {"text": "ch∆∞∆°ng tr√¨nh h·ªçc c·ªßa t√¥i", "expected_intent": "ask_student_program"},
            {"text": "xem ch∆∞∆°ng tr√¨nh h·ªçc c·ªßa m√¨nh", "expected_intent": "ask_student_program"},
        ]
        
    async def test_intent_classification(self) -> Dict:
        """Ki·ªÉm tra ph√¢n lo·∫°i intent"""
        if not self.agent:
            await self.load_agent()
            
        results = {
            "total_tests": len(self.test_cases),
            "correct": 0,
            "incorrect": 0,
            "details": [],
            "accuracy": 0.0
        }
        
        for test_case in self.test_cases:
            try:
                result = await self.agent.parse_message(test_case["text"])
                predicted_intent = result["intent"]["name"]
                confidence = result["intent"]["confidence"]
                expected_intent = test_case["expected_intent"]
                
                is_correct = predicted_intent == expected_intent
                
                if is_correct:
                    results["correct"] += 1
                else:
                    results["incorrect"] += 1
                    
                results["details"].append({
                    "text": test_case["text"],
                    "expected": expected_intent,
                    "predicted": predicted_intent,
                    "confidence": confidence,
                    "correct": is_correct
                })
                
            except Exception as e:
                print(f"‚ùå Error testing '{test_case['text']}': {e}")
                results["details"].append({
                    "text": test_case["text"],
                    "expected": test_case["expected_intent"],
                    "predicted": "ERROR",
                    "confidence": 0.0,
                    "correct": False,
                    "error": str(e)
                })
                results["incorrect"] += 1
                
        results["accuracy"] = results["correct"] / results["total_tests"] if results["total_tests"] > 0 else 0
        return results
        
    async def test_confidence_threshold(self, min_confidence: float = 0.5) -> Dict:
        """Ki·ªÉm tra ng∆∞·ª°ng confidence"""
        if not self.agent:
            await self.load_agent()
            
        low_confidence_cases = []
        
        for test_case in self.test_cases:
            try:
                result = await self.agent.parse_message(test_case["text"])
                confidence = result["intent"]["confidence"]
                
                if confidence < min_confidence:
                    low_confidence_cases.append({
                        "text": test_case["text"],
                        "expected_intent": test_case["expected_intent"],
                        "predicted_intent": result["intent"]["name"],
                        "confidence": confidence
                    })
                    
            except Exception as e:
                print(f"‚ùå Error testing confidence for '{test_case['text']}': {e}")
                
        return {
            "min_confidence": min_confidence,
            "total_tests": len(self.test_cases),
            "low_confidence_count": len(low_confidence_cases),
            "low_confidence_rate": len(low_confidence_cases) / len(self.test_cases),
            "low_confidence_cases": low_confidence_cases
        }
        
    def analyze_intent_distribution(self, results: Dict) -> Dict:
        """Ph√¢n t√≠ch ph√¢n b·ªë intent"""
        intent_stats = {}
        
        for detail in results["details"]:
            expected = detail["expected"]
            predicted = detail["predicted"]
            correct = detail["correct"]
            
            if expected not in intent_stats:
                intent_stats[expected] = {
                    "total": 0,
                    "correct": 0,
                    "accuracy": 0.0,
                    "predictions": []
                }
                
            intent_stats[expected]["total"] += 1
            if correct:
                intent_stats[expected]["correct"] += 1
            intent_stats[expected]["predictions"].append(predicted)
            
        # T√≠nh accuracy cho t·ª´ng intent
        for intent in intent_stats:
            stats = intent_stats[intent]
            stats["accuracy"] = stats["correct"] / stats["total"] if stats["total"] > 0 else 0
            
        return intent_stats
        
    async def run_full_test(self) -> Dict:
        """Ch·∫°y to√†n b·ªô test"""
        print("üß™ B·∫Øt ƒë·∫ßu ki·ªÉm th·ª≠ NLU...")
        
        # Test intent classification
        print("üìã Ki·ªÉm th·ª≠ ph√¢n lo·∫°i intent...")
        intent_results = await self.test_intent_classification()
        
        # Test confidence threshold
        print("üéØ Ki·ªÉm th·ª≠ ng∆∞·ª°ng confidence...")
        confidence_results = await self.test_confidence_threshold(0.5)
        
        # Analyze intent distribution
        print("üìä Ph√¢n t√≠ch ph√¢n b·ªë intent...")
        intent_distribution = self.analyze_intent_distribution(intent_results)
        
        return {
            "intent_classification": intent_results,
            "confidence_analysis": confidence_results,
            "intent_distribution": intent_distribution,
            "summary": self._generate_summary(intent_results, confidence_results, intent_distribution)
        }
        
    def _generate_summary(self, intent_results: Dict, confidence_results: Dict, intent_distribution: Dict) -> Dict:
        """T·∫°o b√°o c√°o t·ªïng k·∫øt"""
        # T√¨m intent c√≥ accuracy th·∫•p nh·∫•t
        worst_intent = min(intent_distribution.items(), 
                          key=lambda x: x[1]["accuracy"]) if intent_distribution else ("none", {"accuracy": 0})
        
        # T√¨m intent c√≥ accuracy cao nh·∫•t
        best_intent = max(intent_distribution.items(), 
                         key=lambda x: x[1]["accuracy"]) if intent_distribution else ("none", {"accuracy": 0})
        
        return {
            "overall_accuracy": intent_results["accuracy"],
            "total_tests": intent_results["total_tests"],
            "correct_predictions": intent_results["correct"],
            "incorrect_predictions": intent_results["incorrect"],
            "low_confidence_rate": confidence_results["low_confidence_rate"],
            "best_performing_intent": {
                "intent": best_intent[0],
                "accuracy": best_intent[1]["accuracy"]
            },
            "worst_performing_intent": {
                "intent": worst_intent[0],
                "accuracy": worst_intent[1]["accuracy"]
            },
            "pass_criteria": {
                "accuracy_threshold": 0.85,
                "confidence_threshold": 0.5,
                "max_low_confidence_rate": 0.1
            },
            "test_status": {
                "accuracy_pass": intent_results["accuracy"] >= 0.85,
                "confidence_pass": confidence_results["low_confidence_rate"] <= 0.1,
                "overall_pass": (intent_results["accuracy"] >= 0.85 and 
                               confidence_results["low_confidence_rate"] <= 0.1)
            }
        }
        
    def save_results(self, results: Dict, output_path: str = "nlu_test_results.json"):
        """L∆∞u k·∫øt qu·∫£ test"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"üìÑ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {output_path}")
        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u k·∫øt qu·∫£: {e}")
            
    def print_summary(self, summary: Dict):
        """In b√°o c√°o t·ªïng k·∫øt"""
        print("\n" + "="*60)
        print("üìä B√ÅO C√ÅO KI·ªÇM TH·ª¨ NLU")
        print("="*60)
        
        print(f"T·ªïng s·ªë test cases: {summary['total_tests']}")
        print(f"D·ª± ƒëo√°n ƒë√∫ng: {summary['correct_predictions']}")
        print(f"D·ª± ƒëo√°n sai: {summary['incorrect_predictions']}")
        print(f"ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ: {summary['overall_accuracy']:.2%}")
        print(f"T·ª∑ l·ªá confidence th·∫•p: {summary['low_confidence_rate']:.2%}")
        
        print(f"\nIntent t·ªët nh·∫•t: {summary['best_performing_intent']['intent']} "
              f"({summary['best_performing_intent']['accuracy']:.2%})")
        print(f"Intent c·∫ßn c·∫£i thi·ªán: {summary['worst_performing_intent']['intent']} "
              f"({summary['worst_performing_intent']['accuracy']:.2%})")
        
        print(f"\nTr·∫°ng th√°i ki·ªÉm th·ª≠:")
        accuracy_status = "‚úÖ PASS" if summary['test_status']['accuracy_pass'] else "‚ùå FAIL"
        confidence_status = "‚úÖ PASS" if summary['test_status']['confidence_pass'] else "‚ùå FAIL"
        overall_status = "‚úÖ PASS" if summary['test_status']['overall_pass'] else "‚ùå FAIL"
        
        print(f"  ƒê·ªô ch√≠nh x√°c (‚â•85%): {accuracy_status}")
        print(f"  Confidence (‚â§10% th·∫•p): {confidence_status}")
        print(f"  T·ªïng th·ªÉ: {overall_status}")
        
        print("="*60)


async def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y test"""
    # ƒê∆∞·ªùng d·∫´n t·ªõi model ƒë√£ train
    model_path = "models/"
    
    # Ki·ªÉm tra xem c√≥ model kh√¥ng
    if not os.path.exists(model_path):
        print("‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c models/")
        print("Vui l√≤ng train model tr∆∞·ªõc khi ch·∫°y test:")
        print("rasa train")
        return
        
    tester = NLUTester(model_path)
    
    try:
        # Ch·∫°y to√†n b·ªô test
        results = await tester.run_full_test()
        
        # In b√°o c√°o
        tester.print_summary(results["summary"])
        
        # L∆∞u k·∫øt qu·∫£ chi ti·∫øt
        tester.save_results(results, "nlu_test_results.json")
        
        # In m·ªôt s·ªë tr∆∞·ªùng h·ª£p sai ƒë·ªÉ debug
        print(f"\nüîç M·ªòT S·ªê TR∆Ø·ªúNG H·ª¢P D·ª∞ ƒêO√ÅN SAI:")
        incorrect_cases = [detail for detail in results["intent_classification"]["details"] 
                          if not detail["correct"]][:5]  # Ch·ªâ in 5 tr∆∞·ªùng h·ª£p ƒë·∫ßu
        
        for case in incorrect_cases:
            print(f"  Text: '{case['text']}'")
            print(f"  Expected: {case['expected']} | Predicted: {case['predicted']} "
                  f"(confidence: {case['confidence']:.3f})")
            print()
            
    except Exception as e:
        print(f"‚ùå L·ªói khi ch·∫°y test: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
